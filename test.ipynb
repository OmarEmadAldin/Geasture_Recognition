{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e782c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.vision import GestureRecognizer , GestureRecognizerOptions , RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ebfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"F:\\Omar 3amora\\Gesture Recognition\\Mediapipe_solution\\gesture_recognizer.task\"  # Replace with your model path\n",
    "\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=RunningMode.VIDEO,\n",
    "    num_hands=1\n",
    ")\n",
    "recognizer = GestureRecognizer.create_from_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1056ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n",
      "Gesture: Thumb_Up\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "frame_counter = 0  # To keep track of timestamps\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a MediaPipe Image\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    # Get timestamp in milliseconds (just use frame counter as dummy timestamp)\n",
    "    timestamp_ms = frame_counter * 33  # Assuming ~30 FPS\n",
    "\n",
    "    # Recognize gestures\n",
    "    recognition_result = recognizer.recognize_for_video(mp_image, timestamp_ms)\n",
    "\n",
    "    # Process result\n",
    "    if recognition_result.gestures:\n",
    "        top_gesture = recognition_result.gestures[0][0].category_name\n",
    "        print(f\"Gesture: {top_gesture}\")\n",
    "        cv2.putText(frame, f\"Gesture: {top_gesture}\", (30, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "    frame_counter += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e759dfc",
   "metadata": {},
   "source": [
    "# Custom Geastures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.vision import GestureRecognizer , GestureRecognizerOptions , RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba436f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef614e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "motion_history = deque(maxlen=5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    motion_text = \"\"\n",
    "    gesture_text = \"\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        lm = hand_landmarks.landmark\n",
    "\n",
    "        # Wrist for motion tracking\n",
    "        wrist = lm[mp_hands.HandLandmark.WRIST]\n",
    "        wrist_pos = np.array([wrist.x * w, wrist.y * h])\n",
    "        motion_history.append(wrist_pos)\n",
    "\n",
    "        # Motion direction\n",
    "        if len(motion_history) >= 2:\n",
    "            delta = motion_history[-1] - motion_history[0]\n",
    "            dx, dy = delta\n",
    "\n",
    "            motion_threshold = 30  # pixels\n",
    "            if abs(dx) > abs(dy):\n",
    "                if dx > motion_threshold:\n",
    "                    motion_text = \"Moving Right\"\n",
    "                elif dx < -motion_threshold:\n",
    "                    motion_text = \"Moving Left\"\n",
    "            else:\n",
    "                if dy > motion_threshold:\n",
    "                    motion_text = \"Moving Down\"\n",
    "                elif dy < -motion_threshold:\n",
    "                    motion_text = \"Moving Up\"\n",
    "\n",
    "        # Gesture detection\n",
    "        # Define finger tip and pip indices\n",
    "        finger_tips = [\n",
    "            mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.PINKY_TIP\n",
    "        ]\n",
    "        finger_pips = [\n",
    "            mp_hands.HandLandmark.INDEX_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.MIDDLE_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.RING_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.PINKY_PIP\n",
    "        ]\n",
    "\n",
    "        # Check which fingers are extended\n",
    "        fingers_extended = []\n",
    "        for tip, pip in zip(finger_tips, finger_pips):\n",
    "            if lm[tip].y < lm[pip].y:\n",
    "                fingers_extended.append(True)\n",
    "            else:\n",
    "                fingers_extended.append(False)\n",
    "\n",
    "        # Determine gestures\n",
    "        if all(fingers_extended):\n",
    "            gesture_text = \"STOP\"\n",
    "        elif all(not f for f in fingers_extended):\n",
    "            gesture_text = \"MOVE BACKWARD\"\n",
    "        elif fingers_extended[0] and fingers_extended[1] and not fingers_extended[2] and not fingers_extended[3]:\n",
    "            gesture_text = \"MOVE FORWARD\"  # Victory sign\n",
    "        else:\n",
    "            gesture_text = \"\"\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display texts\n",
    "        cv2.putText(frame, motion_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, gesture_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Motion and Gestures\", frame)\n",
    "\n",
    "    # Exit on 'q'\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2431e",
   "metadata": {},
   "source": [
    "# Simple Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa515f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Motion Analyzer\n",
    "motion_history = deque(maxlen=5)\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Robot simulation initial position\n",
    "robot_x, robot_y = 320, 240\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    motion_text = \"\"\n",
    "    gesture_text = \"\"\n",
    "    command = \"\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        lm = hand_landmarks.landmark\n",
    "\n",
    "        # Wrist for motion\n",
    "        wrist = lm[mp_hands.HandLandmark.WRIST]\n",
    "        wrist_pos = np.array([wrist.x * w, wrist.y * h])\n",
    "        motion_history.append(wrist_pos)\n",
    "\n",
    "        if len(motion_history) >= 2:\n",
    "            delta = motion_history[-1] - motion_history[0]\n",
    "            dx, dy = delta\n",
    "\n",
    "            motion_threshold = 30\n",
    "            if abs(dx) > abs(dy):\n",
    "                if dx > motion_threshold:\n",
    "                    motion_text = \"Moving Right\"\n",
    "                elif dx < -motion_threshold:\n",
    "                    motion_text = \"Moving Left\"\n",
    "            else:\n",
    "                if dy > motion_threshold:\n",
    "                    motion_text = \"Moving Down\"\n",
    "                elif dy < -motion_threshold:\n",
    "                    motion_text = \"Moving Up\"\n",
    "\n",
    "        # Gesture detection\n",
    "        finger_tips = [\n",
    "            mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "            mp_hands.HandLandmark.PINKY_TIP\n",
    "        ]\n",
    "        finger_pips = [\n",
    "            mp_hands.HandLandmark.INDEX_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.MIDDLE_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.RING_FINGER_PIP,\n",
    "            mp_hands.HandLandmark.PINKY_PIP\n",
    "        ]\n",
    "\n",
    "        fingers_extended = []\n",
    "        for tip, pip in zip(finger_tips, finger_pips):\n",
    "            if lm[tip].y < lm[pip].y:\n",
    "                fingers_extended.append(True)\n",
    "            else:\n",
    "                fingers_extended.append(False)\n",
    "\n",
    "        if all(fingers_extended):\n",
    "            gesture_text = \"STOP\"\n",
    "            command = \"STOP\"\n",
    "        elif all(not f for f in fingers_extended):\n",
    "            gesture_text = \"MOVE BACKWARD\"\n",
    "            command = \"MOVE_BACKWARD\"\n",
    "        elif fingers_extended[0] and fingers_extended[1] and not fingers_extended[2] and not fingers_extended[3]:\n",
    "            gesture_text = \"MOVE FORWARD\"\n",
    "            command = \"MOVE_FORWARD\"\n",
    "        else:\n",
    "            gesture_text = \"\"\n",
    "            command = \"\"\n",
    "\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Update robot position\n",
    "    if command == \"MOVE_FORWARD\":\n",
    "        robot_y -= 5\n",
    "    elif command == \"MOVE_BACKWARD\":\n",
    "        robot_y += 5\n",
    "    elif motion_text == \"Moving Left\":\n",
    "        robot_x -= 5\n",
    "    elif motion_text == \"Moving Right\":\n",
    "        robot_x += 5\n",
    "    elif command == \"STOP\":\n",
    "        pass  # No movement\n",
    "\n",
    "    # Clamp position\n",
    "    robot_x = np.clip(robot_x, 0, 640)\n",
    "    robot_y = np.clip(robot_y, 0, 480)\n",
    "\n",
    "    # Display camera frame\n",
    "    cv2.putText(frame, motion_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, gesture_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Hand Motion and Gestures\", frame)\n",
    "\n",
    "    # Create simulation window\n",
    "    sim_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    cv2.circle(sim_frame, (robot_x, robot_y), 20, (0, 255, 255), -1)\n",
    "    cv2.putText(sim_frame, f\"X:{robot_x} Y:{robot_y}\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "    cv2.imshow(\"Robot Simulation\", sim_frame)\n",
    "\n",
    "    # Exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c567f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
